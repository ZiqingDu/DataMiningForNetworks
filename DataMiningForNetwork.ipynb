{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the library for the code\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import *\n",
    "from time import time\n",
    "import itertools\n",
    "import gc\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the random walk kernel\n",
    "def kernel1(a1):\n",
    "    a0=np.zeros((a1.shape[0],a1.shape[0]))\n",
    "    mylist=list(np.unique(np.nonzero(a1)))\n",
    "    for i in mylist:\n",
    "        a0[i,i]=1\n",
    "    a2=a1*a1\n",
    "    a3=a2*a1  \n",
    "    a4=a3*a1\n",
    "    F=a0+a1+a2+a3+a4;\n",
    "    return F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function to build graphlets\n",
    "def graphles(data,f,flag=1):\n",
    "    df1=pd.DataFrame()\n",
    "    mylist = []\n",
    "        \n",
    "    #the basis graph includes all the elements in the train dataset\n",
    "    D = nx.DiGraph()\n",
    "    D.add_nodes_from([node for node in f['A']])\n",
    "    D.add_nodes_from([node for node in f['B']])\n",
    "    D.add_nodes_from([node for node in f['C']])\n",
    "    D.add_nodes_from([node for node in f['D']])\n",
    "    D.add_nodes_from([node for node in f['E']])\n",
    "    D.add_nodes_from([node for node in f['F']])\n",
    "    a = list(set(data['A']))\n",
    "    for i in a:\n",
    "        \n",
    "        #activity graphlets\n",
    "        G = nx.DiGraph()\n",
    "        df=data[data['A']==i]\n",
    "        G.add_nodes_from([node for node in df['A']])\n",
    "        G.add_nodes_from([node for node in df['B']])\n",
    "        G.add_nodes_from([node for node in df['C']])\n",
    "        G.add_nodes_from([node for node in df['D']])\n",
    "        G.add_nodes_from([node for node in df['E']])\n",
    "        G.add_nodes_from([node for node in df['F']])\n",
    "        G.add_edges_from([edge for edge in zip(df['A'],df['B'])])\n",
    "        G.add_edges_from([edge for edge in zip(df['B'],df['C'])])\n",
    "        G.add_edges_from([edge for edge in zip(df['c'],df['D'])])\n",
    "        G.add_edges_from([edge for edge in zip(df['D'],df['E'])])\n",
    "        G.add_edges_from([edge for edge in zip(df['E'],df['F'])])                \n",
    "        \n",
    "        #significant nodes\n",
    "        b = list(set(df['B']))\n",
    "        c = list(set(df['C']))\n",
    "        e = list(set(df['E']))\n",
    "        f = list(set(df['F']))\n",
    "        \n",
    "        TNode=[];\n",
    "        tmp='A';\n",
    "        com=2;\n",
    "        for j in b:\n",
    "            x=int(G.in_degree(j))\n",
    "            y=int(G.out_degree(j))\n",
    "            if (x+y>com):\n",
    "                tmp=j\n",
    "                com=x+y\n",
    "        TNode.append(tmp)\n",
    "        com=1;\n",
    "        for j in c:\n",
    "            y=int(G.out_degree(j))\n",
    "            if (y>com):\n",
    "                tmp=j\n",
    "                com=y\n",
    "        TNode.append(tmp)\n",
    "        com=1;\n",
    "        for j in f:\n",
    "            y=int(G.in_degree(j))\n",
    "            if (y>com):\n",
    "                tmp=j\n",
    "                com=y\n",
    "        TNode.append(tmp)        \n",
    "        com=2;\n",
    "        for j in e:\n",
    "            x=int(G.in_degree(j))\n",
    "            y=int(G.out_degree(j))\n",
    "            if (x+y>com):\n",
    "                tmp=j\n",
    "                com=x+y\n",
    "        TNode.append(tmp)  \n",
    "        G.clear()\n",
    "        del G\n",
    "        \n",
    "        #profile graphlets\n",
    "        df2=df;\n",
    "        for index, row  in df.iterrows():\n",
    "            tmp=0\n",
    "            if (row[\"B\"]==TNode[0]):\n",
    "                tmp=tmp+1\n",
    "            if (row[\"C\"]==TNode[1]):\n",
    "                tmp=tmp+1\n",
    "            if (row[\"F\"]==TNode[2]):\n",
    "                tmp=tmp+1\n",
    "            if (row[\"E\"]==TNode[3]):\n",
    "                tmp=tmp+1\n",
    "            if (tmp<2):\n",
    "                df2.drop(index)\n",
    "        \n",
    "        D.add_edges_from([edge for edge in zip(df2['A'],df['B'])])\n",
    "        D.add_edges_from([edge for edge in zip(df2['B'],df['C'])])\n",
    "        D.add_edges_from([edge for edge in zip(df2['C'],df['D'])])\n",
    "        D.add_edges_from([edge for edge in zip(df2['D'],df['E'])])\n",
    "        D.add_edges_from([edge for edge in zip(df2['E'],df['F'])])\n",
    "        \n",
    "        #calculate the feature values for each graphlet\n",
    "        tm=[]\n",
    "        if flag==1: # use the random walk kernel\n",
    "            a1=nx.adj_matrix(D)\n",
    "            cd=kernel1(a1)\n",
    "            tm=cd.flatten().tolist()\n",
    "        else: #use the shortest path kernel\n",
    "            cd=nx.floyd_warshall_numpy(D)\n",
    "            tm=cd.flatten().tolist()\n",
    "        D.remove_edges_from(list(D.edges()))\n",
    "        out = list(itertools.chain.from_iterable(tm))\n",
    "        mylist.append(out)\n",
    "        \n",
    "        df1=pd.concat([df1, df2], axis=0, ignore_index=True) #the significant records\n",
    "        \n",
    "    del D\n",
    "    return df1,mylist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the huge dataset into several blocks and easy to compute\n",
    "def seperate(dt,n):\n",
    "    dtgrouped=dt.groupby(dt['A'])\n",
    "    t=dtgrouped.size()\n",
    "    s=0;\n",
    "    sep=[];\n",
    "    for i in range(0,len(t)):\n",
    "        s=s+t[i]\n",
    "        if (s>n):\n",
    "            sep.append(s)\n",
    "            s=0\n",
    "    return sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runsep(df1,df,n=500,flag=1):\n",
    "    sub=pd.DataFrame()\n",
    "    mylist = []\n",
    "    num=seperate(df1,n)\n",
    "    s=0\n",
    "    t=0\n",
    "    if num==[]:\n",
    "        sub,mylist=graphles(df1,df,flag)\n",
    "    else:\n",
    "        for i in range(0,(len(num)+1)):\n",
    "            if (i==len(num)):\n",
    "                dfcut=df1[s:]\n",
    "            else:\n",
    "                t=s+num[i]\n",
    "                dfcut=df1[s:t]\n",
    "            sub1,list1=graphles(dfcut,df,flag)\n",
    "            sub=pd.concat([sub,sub1])\n",
    "            mylist=mylist+list1\n",
    "            s=t\n",
    "    return sub,mylist\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"annotated-trace.csv\",names=['A','C','B','D','E','Label'])\n",
    "df['A'] = 'a' + df['A'].astype(str)\n",
    "df['B'] = 'b' + df['B'].astype(str)\n",
    "df['F'] = 'f' + df['C'].astype(str)\n",
    "df['C'] = 'c' + df['C'].astype(str)\n",
    "df['D'] = 'd' + df['D'].astype(str)\n",
    "df['E'] = 'e' + df['E'].astype(str)\n",
    "df['Label'] = df['Label'].astype(str)\n",
    "df=df.sort_values(by=['Label', 'A','B','C','E'],ascending=False)\n",
    "df1=df[df['Label']=='normal']\n",
    "df2=df[df['Label']=='anomaly']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1,f1=runsep(df1,df,500)\n",
    "sub2,f2=runsep(df2,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(f1)+len(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=f1+f2\n",
    "t=np.append(np.zeros(len(f1)),np.ones(len(f2)))\n",
    "y=np.transpose([t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classif = svm.SVC(kernel='linear')\n",
    "\n",
    "start = time()\n",
    "classif.fit(X, y.ravel())\n",
    "t11=(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_kernel(X, Y):\n",
    "   return np.dot(X, Y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel=my_kernel)\n",
    "start = time()\n",
    "clf.fit(X, y.ravel())\n",
    "t13=(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t11,\"----time----\",t13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.read_csv(\"not-annotated-trace.csv\",names=['A','C','B','D','E'])\n",
    "dt['A'] = 'a' + dt['A'].astype(str)\n",
    "dt['B'] = 'b' + dt['B'].astype(str)\n",
    "dt['F'] = 'f' + dt['C'].astype(str)\n",
    "dt['C'] = 'c' + dt['C'].astype(str)\n",
    "dt['D'] = 'd' + dt['D'].astype(str)\n",
    "dt['E'] = 'e' + dt['E'].astype(str)\n",
    "dt=dt.sort_values(by=['A','B','C','E'])\n",
    "dt1=dt\n",
    "dt=dt[dt['A'].isin(df['A'])]\n",
    "dt=dt[dt['B'].isin(df['B'])]\n",
    "dt=dt[dt['C'].isin(df['C'])]\n",
    "dt=dt[dt['D'].isin(df['D'])]\n",
    "dt=dt[dt['E'].isin(df['E'])]\n",
    "dt=dt[dt['F'].isin(df['F'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subt1=dt1.loc[dt1.index.difference(dt.index)]\n",
    "subt,ft=runsep(dt,df)\n",
    "subt1,q=runsep(subt1,df)\n",
    "subt=pd.concat([subt,subt1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt=classif.predict(ft)\n",
    "yt=np.append(yt,np.ones(len(set(subt1['A']))))\n",
    "index=0;\n",
    "listsub=list(set(subt['A']))\n",
    "for i in yt:\n",
    "    if (i==1):\n",
    "        idc=listsub[index]\n",
    "        gp=subt[subt['A']==idc]\n",
    "        D = nx.DiGraph()\n",
    "        D.add_edges_from([edge for edge in zip(gp['A'],gp['B'])])\n",
    "        D.add_edges_from([edge for edge in zip(gp['B'],gp['C'])])\n",
    "        D.add_edges_from([edge for edge in zip(gp['C'],gp['D'])])\n",
    "        D.add_edges_from([edge for edge in zip(gp['D'],gp['E'])])\n",
    "        D.add_edges_from([edge for edge in zip(gp['E'],gp['F'])])\n",
    "        plt.clf()\n",
    "        nx.draw(D,with_labels=True, font_weight='bold')\n",
    "        plt.savefig(\"examples%d.jpg\"%(index))\n",
    "    index=index+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset in train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel=my_kernel)\n",
    "clf.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# from: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    import itertools\n",
    "\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Class_names\n",
    "class_names = ['0', '1']\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, clf.predict(X_test))\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shortest path method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already made the code in the upper code. To use it, simply to reload the dataset and make graphlets with the command below. The other steps are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1,f1=runsep(df1[:],df,500,0)\n",
    "sub2,f2=runsep(df2,df,500,0)\n",
    "subt,ft=runsep(dt1,df,500,0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
